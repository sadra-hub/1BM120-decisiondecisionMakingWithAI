{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd19ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cell 0] Device in use: cuda\n",
      "[Cell 0] Loading datasets…\n",
      "[Cell 0] Train set size: 136, Test set size: 34\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Imports, constants, helper functions, CNN class, and data loading\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from support import load_dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# -----------------------------\n",
    "#        INPUT PARAMETERS\n",
    "# -----------------------------\n",
    "# Choose one of: \"random\" or \"TPE\"\n",
    "SEARCH_METHOD = \"random\"    # ← change to \"TPE\" if you want TPE Search\n",
    "\n",
    "# Random seed\n",
    "SEED = 18\n",
    "\n",
    "# Cross-Validation (CV) settings\n",
    "CV_FOLDS  = 5\n",
    "CV_EPOCHS = 30\n",
    "\n",
    "# Optuna hyperparameter tuning settings\n",
    "OPTUNA_TRIALS = 10\n",
    "\n",
    "# Persistent storage URL (one file per method)\n",
    "STORAGE_URL = f\"sqlite:///{SEARCH_METHOD}_hparam_results.db\"\n",
    "\n",
    "# Final model training settings\n",
    "NUM_EPOCHS_FINAL = 30\n",
    "\n",
    "# Hyperparameter search ranges (you can adjust these if desired)\n",
    "HP_LR_LOW    = 1e-5\n",
    "HP_LR_HIGH   = 1e-2\n",
    "HP_DROPOUT_LOW  = 0.1\n",
    "HP_DROPOUT_HIGH = 0.7\n",
    "HP_BATCH_OPTIONS   = [16, 32]\n",
    "HP_NUM_FILTERS_OPTS = [16, 32, 48, 64]\n",
    "\n",
    "# Filenames for saved plots and model checkpoints\n",
    "VAL_CURVES_FILENAME      = \"validation_curves.png\"\n",
    "VAL_LOSS_CURVES_FILENAME = \"validation_loss_curves.png\"\n",
    "# When saving the final retrained model:\n",
    "MODEL_FILENAME = f\"cnn_{SEARCH_METHOD}_best.pth\"\n",
    "\n",
    "# Base directory for plots (one per SEARCH_METHOD)\n",
    "PLOTS_DIR = f\"plots/{SEARCH_METHOD}\"\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "# Other constants\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "#        END PARAMETERS\n",
    "# -----------------------------\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed: int = SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "\n",
    "print(f\"[Cell 0] Device in use: {DEVICE}\")\n",
    "print(\"[Cell 0] Loading datasets…\")\n",
    "train_dataset, test_dataset = load_dataset()\n",
    "print(f\"[Cell 0] Train set size: {len(train_dataset)}, Test set size: {len(test_dataset)}\")\n",
    "\n",
    "# Define CNN (single definition)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_filters: int = 16, dropout: float = 0.5):\n",
    "        super(CNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, num_filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(num_filters, num_filters * 2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        # Dynamically compute flattened size (input assumed 60 × 30)\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, 60, 30)\n",
    "            out = self.features(dummy)\n",
    "            flat_size = out.view(1, -1).shape[1]\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flat_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Training / evaluation helpers\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc  = correct / len(loader.dataset)\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def evaluate_loss(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            count += inputs.size(0)\n",
    "    return total_loss / count\n",
    "\n",
    "# Train a model for multiple epochs (used later for final retraining)\n",
    "def train_full_model(model, train_loader, test_loader, criterion, optimizer, device, num_epochs: int):\n",
    "    train_accs = []\n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "    test_losses = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Train on training set\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc  = correct / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                count += inputs.size(0)\n",
    "        test_loss = total_loss / count\n",
    "        test_acc  = correct / count\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        print(\n",
    "            f\"[Final] Epoch {epoch:02d}/{num_epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "            f\"Test  Loss: {test_loss:.4f}, Test  Acc: {test_acc:.4f}\"\n",
    "        )\n",
    "    return train_losses, train_accs, test_losses, test_accs\n",
    "\n",
    "# Plot learning curves (accuracy + loss) and save under plots/{SEARCH_METHOD}/…\n",
    "def plot_learning_curves(\n",
    "    epochs,\n",
    "    train_accs,\n",
    "    test_accs,\n",
    "    train_losses,\n",
    "    test_losses,\n",
    "    acc_title: str,\n",
    "    loss_title: str,\n",
    "    acc_filepath: str,\n",
    "    loss_filepath: str\n",
    "):\n",
    "    # Accuracy plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, train_accs, marker='o', label=\"Train Accuracy\")\n",
    "    plt.plot(epochs, test_accs, marker='s', label=\"Test Accuracy\")\n",
    "    if train_accs:\n",
    "        best_train_idx = int(np.argmax(train_accs))\n",
    "        best_train_val = max(train_accs)\n",
    "        plt.scatter(epochs[best_train_idx], best_train_val, color='blue')\n",
    "        plt.text(epochs[best_train_idx], best_train_val + 0.01,\n",
    "                 f\"Max Train Acc: {best_train_val:.2f}\", color='blue')\n",
    "    if test_accs:\n",
    "        best_test_idx = int(np.argmax(test_accs))\n",
    "        best_test_val = max(test_accs)\n",
    "        plt.scatter(epochs[best_test_idx], best_test_val, color='orange')\n",
    "        plt.text(epochs[best_test_idx], best_test_val + 0.01,\n",
    "                 f\"Max Test Acc: {best_test_val:.2f}\", color='orange')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(acc_title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(acc_filepath, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Loss plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, train_losses, marker='o', label=\"Train Loss\")\n",
    "    plt.plot(epochs, test_losses, marker='s', label=\"Test Loss\")\n",
    "    if train_losses:\n",
    "        best_train_loss_idx = int(np.argmin(train_losses))\n",
    "        best_train_loss_val = min(train_losses)\n",
    "        plt.scatter(epochs[best_train_loss_idx], best_train_loss_val, color='blue')\n",
    "        plt.text(epochs[best_train_loss_idx], best_train_loss_val + 0.01,\n",
    "                 f\"Min Train Loss: {best_train_loss_val:.2f}\", color='blue')\n",
    "    if test_losses:\n",
    "        best_test_loss_idx = int(np.argmin(test_losses))\n",
    "        best_test_loss_val = min(test_losses)\n",
    "        plt.scatter(epochs[best_test_loss_idx], best_test_loss_val, color='orange')\n",
    "        plt.text(epochs[best_test_loss_idx], best_test_loss_val + 0.01,\n",
    "                 f\"Min Test Loss: {best_test_loss_val:.2f}\", color='orange')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(loss_title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(loss_filepath, dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c0f350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cell 1] Starting RANDOM Search hyperparameter tuning…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:30:20,006] A new study created in RDB with name: random_search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 0] lr=2.40e-04, dropout=0.62, opt=Adam, batch_size=16, num_filters=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-06-04 20:30:49,118] Trial 0 failed with parameters: {'lr': 0.00023986048872438352, 'dropout': 0.6193967519170447, 'optimizer': 'Adam', 'batch_size': 16, 'num_filters': 32} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\flori\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_5500\\3222096377.py\", line 55, in objective\n",
      "    train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
      "  File \"C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_5500\\1564643392.py\", line 114, in train_one_epoch\n",
      "    for inputs, labels in loader:\n",
      "                          ^^^^^^\n",
      "  File \"C:\\Users\\flori\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py\", line 733, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\flori\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1491, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\flori\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1443, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\flori\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1284, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\queue.py\", line 180, in get\n",
      "    self.not_empty.wait(remaining)\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\threading.py\", line 359, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-06-04 20:30:49,202] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 110\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Create or load study\u001b[39;00m\n\u001b[0;32m    103\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m    104\u001b[0m     study_name\u001b[38;5;241m=\u001b[39mstudy_name,\n\u001b[0;32m    105\u001b[0m     storage\u001b[38;5;241m=\u001b[39mSTORAGE_URL,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m     sampler\u001b[38;5;241m=\u001b[39msampler\n\u001b[0;32m    109\u001b[0m )\n\u001b[1;32m--> 110\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39mOPTUNA_TRIALS)\n\u001b[0;32m    112\u001b[0m best_trial   \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n\u001b[0;32m    113\u001b[0m best_params  \u001b[38;5;241m=\u001b[39m best_trial\u001b[38;5;241m.\u001b[39mparams\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     _optimize(\n\u001b[0;32m    476\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    477\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    478\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    479\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    480\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    481\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    482\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    483\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    484\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    485\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[0;32m     64\u001b[0m             study,\n\u001b[0;32m     65\u001b[0m             func,\n\u001b[0;32m     66\u001b[0m             n_trials,\n\u001b[0;32m     67\u001b[0m             timeout,\n\u001b[0;32m     68\u001b[0m             catch,\n\u001b[0;32m     69\u001b[0m             callbacks,\n\u001b[0;32m     70\u001b[0m             gc_after_trial,\n\u001b[0;32m     71\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     72\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     73\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     74\u001b[0m         )\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[2], line 55\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Train for CV_EPOCHS on this fold\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, CV_EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 55\u001b[0m     train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m CV_EPOCHS \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    [Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCV_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 114\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m    112\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    113\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m    115\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    116\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1491\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:1443\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1443\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[0;32m   1444\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1445\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1272\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1284\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   1285\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1286\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1287\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1289\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 1: Run a single hyperparameter search (“random” or “TPE”) and save trials to SQLite\n",
    "\n",
    "print(f\"[Cell 1] Starting {SEARCH_METHOD.upper()} Search hyperparameter tuning…\")\n",
    "\n",
    "# Precompute labels and indices for StratifiedKFold\n",
    "labels_list = [int(train_dataset[i][1]) for i in range(len(train_dataset))]\n",
    "all_indices = list(range(len(train_dataset)))\n",
    "\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters within the ranges defined in Cell 0\n",
    "    lr            = trial.suggest_float(\"lr\", HP_LR_LOW, HP_LR_HIGH, log=True)\n",
    "    dropout       = trial.suggest_float(\"dropout\", HP_DROPOUT_LOW, HP_DROPOUT_HIGH)\n",
    "    optimizer_name= trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"])\n",
    "    batch_size    = trial.suggest_categorical(\"batch_size\", HP_BATCH_OPTIONS)\n",
    "    num_filters   = trial.suggest_categorical(\"num_filters\", HP_NUM_FILTERS_OPTS)\n",
    "\n",
    "    print(\n",
    "        f\"[Trial {trial.number}] lr={lr:.2e}, dropout={dropout:.2f}, \"\n",
    "        f\"opt={optimizer_name}, batch_size={batch_size}, num_filters={num_filters}\"\n",
    "    )\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=SEED)\n",
    "    fold_accuracies = []\n",
    "    fold_losses     = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(all_indices, labels_list), start=1):\n",
    "        train_sub = Subset(train_dataset, train_idx)\n",
    "        val_sub   = Subset(train_dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_sub,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_sub,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        model     = CNN(num_filters=num_filters, dropout=dropout).to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = (\n",
    "            optim.Adam(model.parameters(), lr=lr)\n",
    "            if optimizer_name == \"Adam\"\n",
    "            else optim.SGD(model.parameters(), lr=lr)\n",
    "        )\n",
    "\n",
    "        # Train for CV_EPOCHS on this fold\n",
    "        for epoch in range(1, CV_EPOCHS + 1):\n",
    "            train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "            if epoch == CV_EPOCHS or epoch % 5 == 0:\n",
    "                print(f\"    [Fold {fold_idx}] Epoch {epoch}/{CV_EPOCHS}\")\n",
    "\n",
    "        # Evaluate on validation fold\n",
    "        total_loss = 0.0\n",
    "        correct    = 0\n",
    "        total      = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += inputs.size(0)\n",
    "\n",
    "        mean_loss_fold = total_loss / total\n",
    "        acc_fold       = correct / total\n",
    "\n",
    "        print(\n",
    "            f\"    → Fold {fold_idx} results: Acc={acc_fold:.4f}, Loss={mean_loss_fold:.4f}\"\n",
    "        )\n",
    "        fold_losses.append(mean_loss_fold)\n",
    "        fold_accuracies.append(acc_fold)\n",
    "\n",
    "    mean_cv_acc  = float(np.mean(fold_accuracies))\n",
    "    mean_cv_loss = float(np.mean(fold_losses))\n",
    "    trial.set_user_attr(\"mean_cv_loss\", mean_cv_loss)\n",
    "\n",
    "    print(\n",
    "        f\"[Trial {trial.number}] mean CV Acc={mean_cv_acc:.4f}, mean CV Loss={mean_cv_loss:.4f}\\n\"\n",
    "    )\n",
    "    return mean_cv_acc\n",
    "\n",
    "# Choose sampler based on SEARCH_METHOD\n",
    "if SEARCH_METHOD.lower() == \"random\":\n",
    "    sampler    = optuna.samplers.RandomSampler()\n",
    "    study_name = \"random_search\"\n",
    "elif SEARCH_METHOD.lower() == \"tpe\":\n",
    "    sampler    = optuna.samplers.TPESampler()\n",
    "    study_name = \"tpe_search\"\n",
    "else:\n",
    "    raise ValueError(\"SEARCH_METHOD must be either 'random' or 'TPE'\")\n",
    "\n",
    "# Create or load study\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    storage=STORAGE_URL,\n",
    "    load_if_exists=True,\n",
    "    direction=\"maximize\",\n",
    "    sampler=sampler\n",
    ")\n",
    "study.optimize(objective, n_trials=OPTUNA_TRIALS)\n",
    "\n",
    "best_trial   = study.best_trial\n",
    "best_params  = best_trial.params\n",
    "best_cv_acc  = best_trial.value\n",
    "best_cv_loss = best_trial.user_attrs.get(\"mean_cv_loss\", float(\"nan\"))\n",
    "\n",
    "print(\n",
    "    f\"[Cell 1] {SEARCH_METHOD.upper()} Search best trial #{best_trial.number} | \"\n",
    "    f\"Hyperparams={best_params} | CV Acc={best_cv_acc:.4f} | CV Loss={best_cv_loss:.4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa128565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Plot validation curves & retrain the best‐so‐far model on full data\n",
    "\n",
    "# 1) Create plots/{SEARCH_METHOD} directory if it doesn’t exist (in case Cell 0 wasn’t re‐run)\n",
    "os.makedirs(f\"plots/{SEARCH_METHOD}\", exist_ok=True)\n",
    "\n",
    "print(\"[Cell 2] Loading the saved study from disk…\")\n",
    "study_name = f\"{SEARCH_METHOD.lower()}_search\"\n",
    "study = optuna.load_study(\n",
    "    study_name=study_name,\n",
    "    storage=STORAGE_URL\n",
    ")\n",
    "\n",
    "# 2a) Print all trials with their hyperparameters, CV accuracy & CV loss\n",
    "print(f\"\\n{SEARCH_METHOD.upper()} Search trial summary:\")\n",
    "for t in study.trials:\n",
    "    params = t.params\n",
    "    acc    = t.value if t.value is not None else \"N/A\"\n",
    "    loss   = t.user_attrs.get(\"mean_cv_loss\", \"N/A\")\n",
    "    print(f\"  Trial #{t.number:2d} | Params={params} | CV Acc={acc} | CV Loss={loss}\")\n",
    "\n",
    "# 2b) Extract only the completed trials (i.e. t.value is not None)\n",
    "valid_trials   = [t for t in study.trials if t.value is not None]\n",
    "trial_nums     = [t.number for t in valid_trials]\n",
    "trial_accs     = [t.value  for t in valid_trials]\n",
    "trial_losses   = [t.user_attrs.get(\"mean_cv_loss\", np.nan) for t in valid_trials]\n",
    "\n",
    "# 2c) Scatter-plots: each hyperparameter versus CV accuracy\n",
    "print(f\"\\n[Cell 2] Plotting scatter‐plots of {SEARCH_METHOD.upper()} hyperparameters vs. CV accuracy…\")\n",
    "param_names = list(study.best_params.keys())\n",
    "for param in param_names:\n",
    "    x_vals = [t.params[param] for t in valid_trials]\n",
    "    y_vals = [t.value           for t in valid_trials]\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(x_vals, y_vals, marker='o', edgecolor='k')\n",
    "    plt.xlabel(param)\n",
    "    plt.ylabel(\"Mean CV Accuracy\")\n",
    "    plt.title(f\"{SEARCH_METHOD.capitalize()} Search: {param} vs. CV Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{PLOTS_DIR}/scatter_{SEARCH_METHOD.lower()}_{param}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# 2d) Plot validation-accuracy curve (CV accuracy vs. trial number)\n",
    "print(\"\\n[Cell 2] Plotting validation accuracy curve…\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(trial_nums, trial_accs, marker='o', label=f\"{SEARCH_METHOD.capitalize()} CV Acc\")\n",
    "if trial_accs:\n",
    "    best_idx = trial_nums[np.argmax(trial_accs)]\n",
    "    best_val = max(trial_accs)\n",
    "    plt.scatter(best_idx, best_val, color='blue')\n",
    "    plt.text(best_idx, best_val + 0.005, f\"Max CV Acc: {best_val:.2f}\", color='blue')\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Mean CV Accuracy\")\n",
    "plt.title(f\"{SEARCH_METHOD.capitalize()} Search: Validation Accuracy ({CV_FOLDS}-Fold, {CV_EPOCHS} Epochs)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{PLOTS_DIR}/{VAL_CURVES_FILENAME}\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 2e) Plot validation-loss curve (CV loss vs. trial number)\n",
    "print(\"\\n[Cell 2] Plotting validation loss curve…\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(trial_nums, trial_losses, marker='s', label=f\"{SEARCH_METHOD.capitalize()} CV Loss\")\n",
    "if trial_losses:\n",
    "    best_loss_idx = trial_nums[np.nanargmin(trial_losses)]\n",
    "    best_loss_val = min([v for v in trial_losses if not np.isnan(v)])\n",
    "    plt.scatter(best_loss_idx, best_loss_val, color='orange')\n",
    "    plt.text(best_loss_idx, best_loss_val + 0.005, f\"Min CV Loss: {best_loss_val:.2f}\", color='orange')\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Mean CV Loss\")\n",
    "plt.title(f\"{SEARCH_METHOD.capitalize()} Search: Validation Loss ({CV_FOLDS}-Fold, {CV_EPOCHS} Epochs)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{PLOTS_DIR}/{VAL_LOSS_CURVES_FILENAME}\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 3) Retrain the best-so-far trial on the full training set and plot its learning curves\n",
    "criterion_final = nn.CrossEntropyLoss()\n",
    "\n",
    "def build_model_and_optimizer(params):\n",
    "    model = CNN(num_filters=params[\"num_filters\"], dropout=params[\"dropout\"]).to(DEVICE)\n",
    "    optimizer = (\n",
    "        optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "        if params[\"optimizer\"] == \"Adam\"\n",
    "        else optim.SGD(model.parameters(), lr=params[\"lr\"])\n",
    "    )\n",
    "    return model, optimizer\n",
    "\n",
    "best_trial   = study.best_trial\n",
    "best_params  = best_trial.params\n",
    "best_cv_acc  = best_trial.value\n",
    "best_cv_loss = best_trial.user_attrs.get(\"mean_cv_loss\", float(\"nan\"))\n",
    "\n",
    "print(\n",
    "    f\"\\n[Cell 2] Best‐so‐far {SEARCH_METHOD.upper()} trial #{best_trial.number} | \"\n",
    "    f\"Hyperparams={best_params} | CV Acc={best_cv_acc:.4f} | CV Loss={best_cv_loss:.4f}\"\n",
    ")\n",
    "\n",
    "model_best, opt_best = build_model_and_optimizer(best_params)\n",
    "train_loader_best = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=best_params[\"batch_size\"],  # use tuned batch_size\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=best_params[\"batch_size\"],  # also use tuned batch_size on test if desired\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"[Cell 2] Training {SEARCH_METHOD.capitalize()}-best model on full training set…\")\n",
    "best_train_losses, best_train_accs, best_test_losses, best_test_accs = train_full_model(\n",
    "    model_best,\n",
    "    train_loader_best,\n",
    "    test_loader,\n",
    "    criterion_final,\n",
    "    opt_best,\n",
    "    DEVICE,\n",
    "    NUM_EPOCHS_FINAL\n",
    ")\n",
    "\n",
    "epochs_final = np.arange(1, NUM_EPOCHS_FINAL + 1)\n",
    "print(\"[Cell 2] Plotting learning curves for the best-so-far model…\")\n",
    "plot_learning_curves(\n",
    "    epochs_final,\n",
    "    best_train_accs,\n",
    "    best_test_accs,\n",
    "    best_train_losses,\n",
    "    best_test_losses,\n",
    "    acc_title=f\"{SEARCH_METHOD.capitalize()} Best Model: Train & Test Accuracy per Epoch\",\n",
    "    loss_title=f\"{SEARCH_METHOD.capitalize()} Best Model: Train & Test Loss per Epoch\",\n",
    "    acc_filepath=f\"{PLOTS_DIR}/best_{SEARCH_METHOD.lower()}_learning_curve.png\",\n",
    "    loss_filepath=f\"{PLOTS_DIR}/best_{SEARCH_METHOD.lower()}_loss_curve.png\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
