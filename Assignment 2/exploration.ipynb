{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e075479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# <p align=\"center\">\n",
    "#   <img src=\"https://upload.wikimedia.org/wikipedia/commons/7/78/Eindhoven_University_of_Technology_logo_new.png?20231008195526\" alt=\"TU/e Logo\" width=\"200px\"/>\n",
    "# </p>\n",
    "# \n",
    "# # Assignment 2: CNN, AutoML & Hyperparameter Optimization (with CV Loss & Accuracy Tracking)\n",
    "# \n",
    "# **Course:** 1BM120 – Decision Making with Artificial Intelligence  \n",
    "# **Date:** *Q4 - 2025*  \n",
    "# \n",
    "# ---\n",
    "# \n",
    "# ## Group 3\n",
    "# \n",
    "# - **Sadra Moosavi Lar**  \n",
    "#   ✉️ [s.s.moosavi.lar@student.tue.nl](mailto:s.s.moosavi.lar@student.tue.nl)\n",
    "# \n",
    "# - **Floris van Hasselt**  \n",
    "#   ✉️ [f.j.p.v.hasselt@student.tue.nl](mailto:f.j.p.v.hasselt@student.tue.nl)\n",
    "# \n",
    "# - **Sam Fiers**  \n",
    "#   ✉️ [s.s.w.fiers@student.tue.nl](mailto:s.s.w.fiers@student.tue.nl)\n",
    "# \n",
    "# ---\n",
    "# \n",
    "# **Repository:** [GitHub – Group 3 Repo](https://github.com/sadra-hub/1BM120-decisiondecisionMakingWithAI)\n",
    "# \n",
    "# ---\n",
    "# \n",
    "# ## Description\n",
    "# \n",
    "# We build three workflows:\n",
    "# 1. **Baseline CNN** (no tuning): train on the full training set, track training & test accuracy and loss per epoch, plot learning curves.\n",
    "# 2. **Hyperparameter Tuning** (Random Search vs. TPE Search): perform 5-fold cross-validation inside each trial, train 20 epochs per fold; record mean CV accuracy (for choosing best trial) and record mean CV loss in trial metadata. Plot validation curves for both accuracy and loss.\n",
    "# 3. **Retrain Best Models**: using best hyperparameters from Random Search and TPE Search (based on accuracy), retrain each on the entire training set, track training & test accuracy and loss per epoch, plot learning curves, highlight highest accuracies, save hyperparameters alongside model weights, and save plots in high quality.\n",
    "\n",
    "# %%\n",
    "# -----------------------------\n",
    "#        INPUT PARAMETERS\n",
    "# -----------------------------\n",
    "# Random seed\n",
    "SEED = 18\n",
    "\n",
    "# DataLoader settings\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True\n",
    "\n",
    "# Baseline CNN settings\n",
    "BATCH_SIZE_BASELINE = 16\n",
    "NUM_EPOCHS_BASELINE = 10\n",
    "LR_BASELINE = 0.001\n",
    "\n",
    "# Cross-Validation (CV) settings\n",
    "CV_FOLDS = 5\n",
    "CV_EPOCHS = 20\n",
    "\n",
    "# Optuna hyperparameter tuning settings\n",
    "OPTUNA_TRIALS = 5\n",
    "\n",
    "# Final model training settings\n",
    "NUM_EPOCHS_FINAL = 20\n",
    "BATCH_SIZE_TEST = 16  # For test DataLoader\n",
    "\n",
    "# Other constants\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "VAL_CURVES_FILENAME = \"validation_curves.png\"\n",
    "VAL_LOSS_CURVES_FILENAME = \"validation_loss_curves.png\"\n",
    "BASELINE_CURVE_FILENAME = \"baseline_learning_curve.png\"\n",
    "RANDOM_CURVE_FILENAME = \"random_learning_curve.png\"\n",
    "TPE_CURVE_FILENAME = \"tpe_learning_curve.png\"\n",
    "RANDOM_MODEL_FILENAME = \"cnn_random_search_best.pth\"\n",
    "TPE_MODEL_FILENAME = \"cnn_tpe_search_best.pth\"\n",
    "\n",
    "# -----------------------------\n",
    "#        END PARAMETERS\n",
    "# -----------------------------\n",
    "\n",
    "# Import necessary libraries (after parameters to use DEVICE)\n",
    "import torch\n",
    "import random\n",
    "import optuna\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from support import load_dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "\n",
    "# %%\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed: int = SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# %%\n",
    "# Load datasets using the provided function in support.py\n",
    "train_dataset, test_dataset = load_dataset()\n",
    "\n",
    "# Simple helper to display balanced samples from a DataLoader\n",
    "def show_balanced_samples(loader, classes=[\"OK\", \"Defective\"]):\n",
    "    ok_imgs = []\n",
    "    def_imgs = []\n",
    "    for images, labels in loader:\n",
    "        for img, label in zip(images, labels):\n",
    "            if label == 0 and len(ok_imgs) < 4:\n",
    "                ok_imgs.append(img)\n",
    "            elif label == 1 and len(def_imgs) < 4:\n",
    "                def_imgs.append(img)\n",
    "            if len(ok_imgs) == 4 and len(def_imgs) == 4:\n",
    "                break\n",
    "        if len(ok_imgs) == 4 and len(def_imgs) == 4:\n",
    "            break\n",
    "\n",
    "    ordered_imgs = []\n",
    "    ordered_labels = []\n",
    "    for i in range(4):\n",
    "        ordered_imgs.extend([ok_imgs[i], def_imgs[i]])\n",
    "        ordered_labels.extend([0, 1])\n",
    "\n",
    "    _, H, W = ordered_imgs[0].shape\n",
    "    aspect = W / H\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    axes = axes.flatten()\n",
    "    for idx in range(8):\n",
    "        img = ordered_imgs[idx].permute(1, 2, 0).numpy()\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(classes[ordered_labels[idx]])\n",
    "        axes[idx].axis(\"off\")\n",
    "        axes[idx].set_aspect(aspect)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display a few samples from the training set\n",
    "train_loader_for_display = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE_BASELINE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "show_balanced_samples(train_loader_for_display)\n",
    "\n",
    "# Define a single CNN class, parameterized by num_filters (for first conv layer) and dropout rate\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_filters: int = 16, dropout: float = 0.5):\n",
    "        super(CNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, num_filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                  # Output: (num_filters, H/2, W/2)\n",
    "            nn.Conv2d(num_filters, num_filters * 2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                   # Output: (num_filters*2, H/4, W/4)\n",
    "        )\n",
    "        # Dynamically compute flattened size after convolutions (assuming input 60×30)\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, 60, 30)\n",
    "            out = self.features(dummy)\n",
    "            flat_size = out.view(1, -1).shape[1]\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flat_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# %%\n",
    "# Helper functions: train for one epoch, evaluate on accuracy and loss\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = correct / len(loader.dataset)\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def evaluate_loss(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            count += inputs.size(0)\n",
    "    mean_loss = total_loss / count\n",
    "    return mean_loss\n",
    "\n",
    "# %%\n",
    "# Helper to train a model for multiple epochs, tracking training & test accuracy and loss per epoch\n",
    "def train_full_model(model, train_loader, test_loader, criterion, optimizer, device, num_epochs: int):\n",
    "    train_accs = []\n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "    test_losses = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss = evaluate_loss(model, test_loader, criterion, device)\n",
    "        test_acc = evaluate(model, test_loader, device)\n",
    "\n",
    "        train_accs.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    return train_losses, train_accs, test_losses, test_accs\n",
    "\n",
    "# %%\n",
    "# Function to plot accuracy and loss learning curves and save in high quality\n",
    "def plot_learning_curves(\n",
    "    epochs,\n",
    "    train_accs,\n",
    "    test_accs,\n",
    "    train_losses,\n",
    "    test_losses,\n",
    "    acc_title: str,\n",
    "    loss_title: str,\n",
    "    acc_filename: str,\n",
    "    loss_filename: str\n",
    "):\n",
    "    # Accuracy plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, train_accs, marker='o', label=\"Train Accuracy\")\n",
    "    plt.plot(epochs, test_accs, marker='s', label=\"Test Accuracy\")\n",
    "    # Highlight highest train and test\n",
    "    best_train_idx = int(np.argmax(train_accs))\n",
    "    best_train_val = max(train_accs)\n",
    "    plt.scatter(epochs[best_train_idx], best_train_val, color='blue')\n",
    "    plt.text(\n",
    "        epochs[best_train_idx],\n",
    "        best_train_val + 0.01,\n",
    "        f\"Max Train Acc: {best_train_val:.2f}\",\n",
    "        color='blue'\n",
    "    )\n",
    "    best_test_idx = int(np.argmax(test_accs))\n",
    "    best_test_val = max(test_accs)\n",
    "    plt.scatter(epochs[best_test_idx], best_test_val, color='orange')\n",
    "    plt.text(\n",
    "        epochs[best_test_idx],\n",
    "        best_test_val + 0.01,\n",
    "        f\"Max Test Acc: {best_test_val:.2f}\",\n",
    "        color='orange'\n",
    "    )\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(acc_title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(acc_filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Loss plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, train_losses, marker='o', label=\"Train Loss\")\n",
    "    plt.plot(epochs, test_losses, marker='s', label=\"Test Loss\")\n",
    "    # Highlight lowest train and test\n",
    "    best_train_loss_idx = int(np.argmin(train_losses))\n",
    "    best_train_loss_val = min(train_losses)\n",
    "    plt.scatter(epochs[best_train_loss_idx], best_train_loss_val, color='blue')\n",
    "    plt.text(\n",
    "        epochs[best_train_loss_idx],\n",
    "        best_train_loss_val + 0.01,\n",
    "        f\"Min Train Loss: {best_train_loss_val:.2f}\",\n",
    "        color='blue'\n",
    "    )\n",
    "    best_test_loss_idx = int(np.argmin(test_losses))\n",
    "    best_test_loss_val = min(test_losses)\n",
    "    plt.scatter(epochs[best_test_loss_idx], best_test_loss_val, color='orange')\n",
    "    plt.text(\n",
    "        epochs[best_test_loss_idx],\n",
    "        best_test_loss_val + 0.01,\n",
    "        f\"Min Test Loss: {best_test_loss_val:.2f}\",\n",
    "        color='orange'\n",
    "    )\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(loss_title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(loss_filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# %%\n",
    "# --------- 1. BASELINE CNN (No Hyperparameter Tuning) ---------\n",
    "# Train on entire training set, evaluate on test set each epoch, plot learning curves\n",
    "\n",
    "baseline_train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE_BASELINE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "baseline_test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE_BASELINE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "baseline_model = CNN(num_filters=16, dropout=0.5).to(DEVICE)\n",
    "criterion_baseline = nn.CrossEntropyLoss()\n",
    "optimizer_baseline = optim.Adam(baseline_model.parameters(), lr=LR_BASELINE)\n",
    "\n",
    "print(\"\\nTraining baseline CNN (no hyperparameter tuning)...\")\n",
    "baseline_train_losses, baseline_train_accs, baseline_test_losses, baseline_test_accs = train_full_model(\n",
    "    baseline_model,\n",
    "    baseline_train_loader,\n",
    "    baseline_test_loader,\n",
    "    criterion_baseline,\n",
    "    optimizer_baseline,\n",
    "    DEVICE,\n",
    "    NUM_EPOCHS_BASELINE\n",
    ")\n",
    "\n",
    "epochs_baseline = np.arange(1, NUM_EPOCHS_BASELINE + 1)\n",
    "plot_learning_curves(\n",
    "    epochs=epochs_baseline,\n",
    "    train_accs=baseline_train_accs,\n",
    "    test_accs=baseline_test_accs,\n",
    "    train_losses=baseline_train_losses,\n",
    "    test_losses=baseline_test_losses,\n",
    "    acc_title=\"Baseline CNN: Train & Test Accuracy per Epoch\",\n",
    "    loss_title=\"Baseline CNN: Train & Test Loss per Epoch\",\n",
    "    acc_filename=BASELINE_CURVE_FILENAME,\n",
    "    loss_filename=\"baseline_loss_curve.png\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nBaseline highest training accuracy: {max(baseline_train_accs):.4f} \"\n",
    "    f\"(Epoch {np.argmax(baseline_train_accs) + 1})\"\n",
    ")\n",
    "print(\n",
    "    f\"Baseline highest test accuracy:     {max(baseline_test_accs):.4f} \"\n",
    "    f\"(Epoch {np.argmax(baseline_test_accs) + 1})\"\n",
    ")\n",
    "print(\n",
    "    f\"Baseline lowest training loss:      {min(baseline_train_losses):.4f} \"\n",
    "    f\"(Epoch {np.argmin(baseline_train_losses) + 1})\"\n",
    ")\n",
    "print(\n",
    "    f\"Baseline lowest test loss:          {min(baseline_test_losses):.4f} \"\n",
    "    f\"(Epoch {np.argmin(baseline_test_losses) + 1})\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# --------- 2. HYPERPARAMETER TUNING (Random Search vs. TPE Search) ---------\n",
    "# We perform CV_FOLDS-fold stratified cross-validation inside each trial, training CV_EPOCHS per fold.\n",
    "# We track both mean CV accuracy (for selecting best trial) and mean CV loss (stored in trial metadata).\n",
    "\n",
    "# Precompute labels for StratifiedKFold\n",
    "labels_list = [int(train_dataset[i][1]) for i in range(len(train_dataset))]\n",
    "all_indices = list(range(len(train_dataset)))\n",
    "\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.7)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32])\n",
    "    num_filters = trial.suggest_int(\"num_filters\", 16, 64, step=16)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=SEED)\n",
    "    fold_accuracies = []\n",
    "    fold_losses = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(\n",
    "        skf.split(all_indices, labels_list), start=1\n",
    "    ):\n",
    "        # Create subsets for this fold\n",
    "        train_sub = Subset(train_dataset, train_idx)\n",
    "        val_sub = Subset(train_dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_sub,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_sub,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY\n",
    "        )\n",
    "\n",
    "        model = CNN(num_filters=num_filters, dropout=dropout).to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = (\n",
    "            optim.Adam(model.parameters(), lr=lr)\n",
    "            if optimizer_name == \"Adam\"\n",
    "            else optim.SGD(model.parameters(), lr=lr)\n",
    "        )\n",
    "\n",
    "        # Train for CV_EPOCHS on this fold\n",
    "        for _epoch in range(1, CV_EPOCHS + 1):\n",
    "            train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "\n",
    "        # Evaluate on validation fold: compute both loss and accuracy\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += inputs.size(0)\n",
    "\n",
    "        mean_loss_fold = total_loss / total\n",
    "        acc_fold = correct / total\n",
    "\n",
    "        fold_losses.append(mean_loss_fold)\n",
    "        fold_accuracies.append(acc_fold)\n",
    "\n",
    "    mean_cv_acc = float(np.mean(fold_accuracies))\n",
    "    mean_cv_loss = float(np.mean(fold_losses))\n",
    "    # Store mean CV loss in trial metadata\n",
    "    trial.set_user_attr(\"mean_cv_loss\", mean_cv_loss)\n",
    "    return mean_cv_acc\n",
    "\n",
    "# Run Random Search\n",
    "random_study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.RandomSampler()\n",
    ")\n",
    "random_study.optimize(objective, n_trials=OPTUNA_TRIALS)\n",
    "\n",
    "# Run TPE Search\n",
    "tpe_study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler()\n",
    ")\n",
    "tpe_study.optimize(objective, n_trials=OPTUNA_TRIALS)\n",
    "\n",
    "print(f\"\\nBest hyperparameters (Random Search): {random_study.best_params}\")\n",
    "print(f\"Best CV accuracy (Random Search):   {random_study.best_value:.4f}\")\n",
    "print(f\"Mean CV loss for that trial:        {random_study.best_trial.user_attrs['mean_cv_loss']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest hyperparameters (TPE Search):  {tpe_study.best_params}\")\n",
    "print(f\"Best CV accuracy (TPE Search):      {tpe_study.best_value:.4f}\")\n",
    "print(f\"Mean CV loss for that trial:        {tpe_study.best_trial.user_attrs['mean_cv_loss']:.4f}\")\n",
    "\n",
    "# Extract trial numbers, CV accuracies, and CV losses for plotting\n",
    "rand_trial_nums = [t.number for t in random_study.trials]\n",
    "rand_trial_accs = [t.value for t in random_study.trials]\n",
    "rand_trial_losses = [t.user_attrs[\"mean_cv_loss\"] for t in random_study.trials]\n",
    "\n",
    "tpe_trial_nums = [t.number for t in tpe_study.trials]\n",
    "tpe_trial_accs = [t.value for t in tpe_study.trials]\n",
    "tpe_trial_losses = [t.user_attrs[\"mean_cv_loss\"] for t in tpe_study.trials]\n",
    "\n",
    "# Plot validation curves (CV accuracy vs. trial) for both searches\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(rand_trial_nums, rand_trial_accs, marker='o', label=\"Random Search CV Acc\")\n",
    "plt.plot(tpe_trial_nums, tpe_trial_accs, marker='s', label=\"TPE Search CV Acc\")\n",
    "\n",
    "# Highlight best points by accuracy\n",
    "best_rand_idx = rand_trial_nums[np.argmax(rand_trial_accs)]\n",
    "best_rand_val = max(rand_trial_accs)\n",
    "plt.scatter(best_rand_idx, best_rand_val, color='blue')\n",
    "plt.text(\n",
    "    best_rand_idx,\n",
    "    best_rand_val + 0.005,\n",
    "    f\"Max Random Acc: {best_rand_val:.2f}\",\n",
    "    color='blue'\n",
    ")\n",
    "\n",
    "best_tpe_idx = tpe_trial_nums[np.argmax(tpe_trial_accs)]\n",
    "best_tpe_val = max(tpe_trial_accs)\n",
    "plt.scatter(best_tpe_idx, best_tpe_val, color='orange')\n",
    "plt.text(\n",
    "    best_tpe_idx,\n",
    "    best_tpe_val + 0.005,\n",
    "    f\"Max TPE Acc: {best_tpe_val:.2f}\",\n",
    "    color='orange'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Mean CV Accuracy\")\n",
    "plt.title(\n",
    "    f\"Hyperparameter Tuning (Accuracy) \"\n",
    "    f\"({CV_FOLDS}-Fold CV, {CV_EPOCHS} Epochs/Fold)\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(VAL_CURVES_FILENAME, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot validation loss curves (CV loss vs. trial) for both searches\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(rand_trial_nums, rand_trial_losses, marker='o', label=\"Random Search CV Loss\")\n",
    "plt.plot(tpe_trial_nums, tpe_trial_losses, marker='s', label=\"TPE Search CV Loss\")\n",
    "\n",
    "# Highlight best points by loss (minimum)\n",
    "best_rand_loss_idx = rand_trial_nums[np.argmin(rand_trial_losses)]\n",
    "best_rand_loss_val = min(rand_trial_losses)\n",
    "plt.scatter(best_rand_loss_idx, best_rand_loss_val, color='blue')\n",
    "plt.text(\n",
    "    best_rand_loss_idx,\n",
    "    best_rand_loss_val + 0.005,\n",
    "    f\"Min Random Loss: {best_rand_loss_val:.2f}\",\n",
    "    color='blue'\n",
    ")\n",
    "\n",
    "best_tpe_loss_idx = tpe_trial_nums[np.argmin(tpe_trial_losses)]\n",
    "best_tpe_loss_val = min(tpe_trial_losses)\n",
    "plt.scatter(best_tpe_loss_idx, best_tpe_loss_val, color='orange')\n",
    "plt.text(\n",
    "    best_tpe_loss_idx,\n",
    "    best_tpe_loss_val + 0.005,\n",
    "    f\"Min TPE Loss: {best_tpe_loss_val:.2f}\",\n",
    "    color='orange'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Mean CV Loss\")\n",
    "plt.title(\n",
    "    f\"Hyperparameter Tuning (Loss) \"\n",
    "    f\"({CV_FOLDS}-Fold CV, {CV_EPOCHS} Epochs/Fold)\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(VAL_LOSS_CURVES_FILENAME, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# --------- 3. RETRAIN BEST MODELS ON FULL TRAINING SET & EVALUATE ---------\n",
    "# We use best hyperparameters from Random Search and TPE Search (chosen by CV accuracy), train each model for multiple epochs,\n",
    "# track training & test accuracy and loss per epoch, plot learning curves, and save hyperparameters alongside model weights.\n",
    "\n",
    "criterion_final = nn.CrossEntropyLoss()\n",
    "\n",
    "# DataLoader for test set\n",
    "final_test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "# Helper to build model + optimizer given hyperparameters\n",
    "def build_model_optimizer(params: dict):\n",
    "    model = CNN(\n",
    "        num_filters=params[\"num_filters\"],\n",
    "        dropout=params[\"dropout\"]\n",
    "    ).to(DEVICE)\n",
    "    optimizer = (\n",
    "        optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "        if params[\"optimizer\"] == \"Adam\"\n",
    "        else optim.SGD(model.parameters(), lr=params[\"lr\"])\n",
    "    )\n",
    "    return model, optimizer\n",
    "\n",
    "# 3a) Random Search best model\n",
    "best_params_random = random_study.best_params\n",
    "print(f\"\\nRetraining Random-Search-best model with params: {best_params_random}\")\n",
    "\n",
    "model_random, optimizer_random = build_model_optimizer(best_params_random)\n",
    "full_train_loader_random = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=best_params_random[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "print(\"Training Random-Search best model on full training set...\")\n",
    "rand_train_losses, rand_train_accs, rand_test_losses, rand_test_accs = train_full_model(\n",
    "    model_random,\n",
    "    full_train_loader_random,\n",
    "    final_test_loader,\n",
    "    criterion_final,\n",
    "    optimizer_random,\n",
    "    DEVICE,\n",
    "    NUM_EPOCHS_FINAL\n",
    ")\n",
    "\n",
    "epochs_final = np.arange(1, NUM_EPOCHS_FINAL + 1)\n",
    "plot_learning_curves(\n",
    "    epochs=epochs_final,\n",
    "    train_accs=rand_train_accs,\n",
    "    test_accs=rand_test_accs,\n",
    "    train_losses=rand_train_losses,\n",
    "    test_losses=rand_test_losses,\n",
    "    acc_title=\"Random-Search Best Model: Train & Test Accuracy per Epoch\",\n",
    "    loss_title=\"Random-Search Best Model: Train & Test Loss per Epoch\",\n",
    "    acc_filename=RANDOM_CURVE_FILENAME,\n",
    "    loss_filename=\"random_loss_curve.png\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nRandom-Search Best Model highest training accuracy: {max(rand_train_accs):.4f} \"\n",
    "    f\"(Epoch {np.argmax(rand_train_accs) + 1})\"\n",
    ")\n",
    "print(\n",
    "    f\"Random-Search Best Model highest test accuracy:     {max(rand_test_accs):.4f} \"\n",
    "    f\"(Epoch {np.argmax(rand_test_accs) + 1})\"\n",
    ")\n",
    "print(\n",
    "    f\"Random-Search Best Model lowest training loss:      {min(rand_train_losses):.4f} \"\n",
    "    f\"(Epoch {np.argmin(rand_train_losses) + 1})\"\n",
    ")\n",
    "print(\n",
    "    f\"Random-Search Best Model lowest test loss:          {min(rand_test_losses):.4f} \"\n",
    "    f\"(Epoch {np.argmin(rand_test_losses) + 1})\"\n",
    ")\n",
    "\n",
    "# Save Random best model checkpoint with hyperparameters\n",
    "torch.save({\n",
    "    \"model_state_dict\": model_random.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer_random.state_dict(),\n",
    "    \"hyperparameters\": best_params_random\n",
    "}, RANDOM_MODEL_FILENAME)\n",
    "\n",
    "# 3b) TPE Search best model\n",
    "best_params_tpe = tpe_study.best_params\n",
    "print(f\"\\nRetraining TPE-Search-best model with params: {best_params_tpe}\")\n",
    "\n",
    "model_tpe, optimizer_tpe = build_model_optimizer(best_params_tpe)\n",
    "full_train_loader_tpe = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=best_params_tpe[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "print(\"Training TPE-Search best model on full training set...\")\n",
    "tpe_train_losses, tpe_train_accs, tpe_test_losses, tpe_test_accs = train_full_model(\n",
    "    model_tpe,\n",
    "    full_train_loader_tpe,\n",
    "    final_test_loader,\n",
    "    criterion_final,\n",
    "    optimizer_tpe,\n",
    "    DEVICE,\n",
    "    NUM_EPOCHS_FINAL\n",
    ")\n",
    "\n",
    "plot_learning_curves(\n",
    "    epochs=epochs_final,\n",
    "    train_accs=tpe_train_accs,\n",
    "    test_accs=tpe_test_accs,\n",
    "    train_losses=tpe_train_losses,\n",
    "    test_losses=tpe_test_losses,\n",
    "    acc_title=\"TPE-Search Best Model: Train & Test Accuracy per Epoch\",\n",
    "    loss_title=\"TPE-Search Best Model: Train & Test Loss per Epoch\",\n",
    "    acc_filename=TPE_CURVE_FILENAME,\n",
    "    loss_filename=\"tpe_loss_curve.png\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nTPE-Search Best Model highest training accuracy: {max(tpe_train_accs):.4f} \"\n",
    "    f\"(Epoch {np.argmax(tpe_train_accs) + 1})\"\n",
    ")\n",
    "print(\n",
    "    f\"TPE-Search Best Model highest test accuracy:     {max(tpe_test_accs):.4f} \"\n",
    "    f\"(Epoch {np.argmax(tpe_test_accs) + 1})\"\n",
    ")\n",
    "print(\n",
    "    f\"TPE-Search Best Model lowest training loss:      {min(tpe_train_losses):.4f} \"\n",
    "    f\"(Epoch {np.argmin(tpe_train_losses) + 1})\"\n",
    ")\n",
    "print(\n",
    "    f\"TPE-Search Best Model lowest test loss:          {min(tpe_test_losses):.4f} \"\n",
    "    f\"(Epoch {np.argmin(tpe_test_losses) + 1})\"\n",
    ")\n",
    "\n",
    "# Save TPE best model checkpoint with hyperparameters\n",
    "torch.save({\n",
    "    \"model_state_dict\": model_tpe.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer_tpe.state_dict(),\n",
    "    \"hyperparameters\": best_params_tpe\n",
    "}, TPE_MODEL_FILENAME)\n",
    "\n",
    "print(\"\\nFinal models and plots saved:\")\n",
    "print(f\"  • {RANDOM_MODEL_FILENAME}\")\n",
    "print(f\"  • {TPE_MODEL_FILENAME}\")\n",
    "print(f\"  • {BASELINE_CURVE_FILENAME}\")\n",
    "print(f\"  • baseline_loss_curve.png\")\n",
    "print(f\"  • {VAL_CURVES_FILENAME}\")\n",
    "print(f\"  • {VAL_LOSS_CURVES_FILENAME}\")\n",
    "print(f\"  • {RANDOM_CURVE_FILENAME}\")\n",
    "print(f\"  • random_loss_curve.png\")\n",
    "print(f\"  • {TPE_CURVE_FILENAME}\")\n",
    "print(f\"  • tpe_loss_curve.png\")\n",
    "\n",
    "# End of script"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
